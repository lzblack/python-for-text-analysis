{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting and processing data\n",
    "\n",
    "This week, we will cover the topic of getting and processing data. Given a research problem, where can you find the relevant data? How do you obtain the data? And how do you actually process the data? This notebook aims to guide you through the process.\n",
    "\n",
    "**At the end of this week, you will be able to:**\n",
    "- Get data from the web using an API or the `requests` library.\n",
    "- Process data from the web using different tools.\n",
    "- Use generator functions.\n",
    "- Use `try` and `except` to handle errors.\n",
    "\n",
    "**This requires that you already have (some) knowledge about:**\n",
    "- JSON files\n",
    "- files, loops and functions\n",
    "\n",
    "**If you want to learn more about these topics, you might find the following links useful:**\n",
    "- Video: [Loop like a native](http://nedbatchelder.com/text/iter.html)\n",
    "\n",
    "**Important**: Please install the following modules before class. For this, you'll need to use the command line environment. On a Mac, use the Terminal application. On Windows, use cmd ([see this video](https://www.youtube.com/watch?v=EohzkYPV6nI)).\n",
    "- GeoPy -- type: `pip install geopy`\n",
    "- pyspotlight -- type: `pip install pyspotlight`\n",
    "- SpaCy\n",
    "\n",
    "To install SpaCy, enter the following commands on the command line.\n",
    "\n",
    "* `conda config --add channels spacy` on the command line\n",
    "* `conda install spacy` \n",
    "* `python -m spacy.en.download` \n",
    "\n",
    "If you're on windows, the last command might give an error. Don't worry, you just need to change the filename of the `en-1.1.0.tmp` folder manually. Go to `YOUR_ANACONDA_FOLDER\\lib\\site-packages\\spacy\\data\\en-1.1.0.tmp` and remove `.tmp` from the filename.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries: generators and error handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has two sides: \n",
    "\n",
    "1. a theoretical side in which we'll cover some important programming concepts, and\n",
    "2. a more practical side in which we'll explore APIs and processing data using NLP tools.\n",
    "\n",
    "We'll first focus on the theory, and then apply that theory in the second half of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now you've probably seen several different error messages. But to be sure, let's execute some broken code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "capitals_dict = {\"The Netherlands\": \"Amsterdam\"}\n",
    "print(capitals_dict[\"France\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "Oh my! ðŸ˜± Python's complaining! There are two ways around this error. Here is the first, familiar way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "country = \"France\"\n",
    "# Check if the country is in the dictionary before getting the associated value.\n",
    "if country in capitals_dict:\n",
    "    print(capitals_dict[country])\n",
    "# If that's not the case, do something else:\n",
    "else:\n",
    "    print(\"Country not in the dictionary!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is another way, using `try` and `except`-statements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Just try to look up the capital of the country.\n",
    "try:\n",
    "    print(capitals_dict[country])\n",
    "# Except if that fails, then print something.\n",
    "except KeyError:\n",
    "    print(\"Country not in the dictionary!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between these two is that in the former, you check whether it's OK to execute the first bit of code before actually going ahead and running it. This is called the \"look before you leap\" coding style ([LBYL](https://docs.python.org/3.6/glossary.html#term-lbyl)). The alternative is to just run the code, and see if it breaks down. If the code breaks down, then you execute some other piece of code. This coding style is associated with the slogan \"It's easier to ask for forgiveness than for permission\" ([EAFP](https://docs.python.org/3.6/glossary.html#term-eafp)).\n",
    "\n",
    "So when do you use which style? Basically, it comes down to these two questions:\n",
    "\n",
    "* How often does the exception happen? If the exception is common, then using the if-statement is better. But if exceptions are rare, then it's better to just run the code and catch the error with the `except`-statement. (Else you'd be performing loads of unnecessary checks.)\n",
    "* How costly is the operation that might give you an error? If it's a very heavy operation, you might want to make sure whether it's OK to run it in the first place. But if the operation is very light, then that's not a very big issue.\n",
    "\n",
    "Read more about errors [here](https://docs.python.org/3.6/tutorial/errors.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generators are functions that produce items one at a time, and forgets each item immediately after producing it, moving to the next one. This is very memory-efficient, because your computer doesn't have to keep a list with all results in memory.\n",
    "\n",
    "OK, that was an abstract definition. Let's see some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def awesome_counter(n):\n",
    "    \"Generator that produces all the whole numbers up to n.\"\n",
    "    # Keep running until the counter has reached n.\n",
    "    for n in range(n):\n",
    "        # Perform any operation you want.\n",
    "        awesome_string = \"The number %s is awesome!\" % str(n)\n",
    "        \n",
    "        # Produce the current value of the counter.\n",
    "        yield awesome_string\n",
    "\n",
    "for message in awesome_counter(10):\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "At each point in time, `count` only refers to one number. Each iteration of the for-loop, `awesome_counter` produces the current value of `awesome_string`, but it doesn't remember the value! This is different from a function like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def awesome_list_counter(n):\n",
    "    \"Function that produces a list with all the whole numbers up to n.\"\n",
    "    # Initialize results list. This is where ALL results will be stored, which will take a lot of memory\n",
    "    # for large values of N.\n",
    "    numbers = []\n",
    "    for n in range(10):\n",
    "        # Perform any operation you want.\n",
    "        awesome_string = \"The number %s is awesome!\" & n\n",
    "\n",
    "        # Append the current value of the counter to the list.\n",
    "        numbers.append(count)\n",
    "\n",
    "    # Return the full result.\n",
    "    return numbers\n",
    "\n",
    "# Here, the function first produces a list, which Python keeps in memory for the duration of the loop.\n",
    "# Afterwards, the list is removed from memory again. But for a short period of time, it's taking up space.\n",
    "for message in awesome_list_counter(10):\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you call a generator function, it returns a *generator object*. You can use the built-in function `next()` to keep calling the next-to-be-generated value from the generator object until it has produced everything it should. At that point, calling `next()` will result in a `StopIteration` error. Please run the next bit of code to see it in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generator = awesome_counter(2)\n",
    "\n",
    "i = next(generator)\n",
    "print(\"the first value is\", i)\n",
    "\n",
    "i = next(generator)\n",
    "print(\"the second value is\", i)\n",
    "\n",
    "i = next(generator)\n",
    "print(\"the third value is\", i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how does the for-loop know when to stop if calling `next()` gives an error at some point? Simple: error handling! Implicitly, the loop looks sort of like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generator = awesome_counter(2)\n",
    "# While loops work like this: the while-statement indicates that you want to keep doing \n",
    "# something while the condition following the 'while'-keyword is true. \n",
    "#\n",
    "# While True means that the loop will never finish, because the condition is never False.\n",
    "while True:\n",
    "    try:\n",
    "        # Try to get the next item.\n",
    "        i = next(generator)\n",
    "    except StopIteration:\n",
    "        print(\"Finishing the loop!\")\n",
    "        # Break out of the loop.\n",
    "        break\n",
    "    \n",
    "    # ...Continue the current iteration.\n",
    "    # Do whatever you want with the item, in this case print it.\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files also work like generators. You can run through them line by line, so that you never have to keep the entire file in memory. Just the current line (and whatever you decide to extract or compute from the text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('../Data/RedCircle/RedCircle.txt')\n",
    "\n",
    "line = next(f)\n",
    "print(line)\n",
    "\n",
    "# Let's move to the end of the file to show we get the same error if we use next() one more time after that.\n",
    "for line in f:\n",
    "    # Do nothing. Just loop until the end of the file.\n",
    "    pass\n",
    "\n",
    "# If you just use next(f), you get an error now.\n",
    "try:\n",
    "    line = next(f)\n",
    "    print(\"Another line!\")\n",
    "except StopIteration:\n",
    "    print(\"Reached the end!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if you only want to read the first N lines, you could use a loop like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "line_number = 0\n",
    "# Open the file.\n",
    "with open('../Data/RedCircle/RedCircle.txt') as f:\n",
    "    # Loop over the file, line by line.\n",
    "    for line in f:\n",
    "        # Print the line\n",
    "        print(line_number, line)\n",
    "        # Increase the line counter\n",
    "        line_number += 1\n",
    "        # And break out of the loop after 10 lines.\n",
    "        if line_number == 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But more Pythonic (prettier code) would be to use `enumerate()`, which also acts like a generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../Data/RedCircle/RedCircle.txt') as f:\n",
    "    # Loop over the file, line by line.\n",
    "    for line_number, line in enumerate(f):\n",
    "        # SELF-CHECK QUESTION:\n",
    "        # Why did I put the if-statement at the beginning now?\n",
    "        if line_number == 10:\n",
    "            break\n",
    "        # Print the line\n",
    "        print(line_number, line)\n",
    "        # And break out of the loop after 10 lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does that for-loop work? Remember that we could use *multiple assignment* to assign values to multiple variables at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x,y = (1,2)\n",
    "print(\"The value of X is:\", x)\n",
    "print(\"The value of Y is:\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same in a for-loop! Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "words = \"I like dogs and cats\"\n",
    "tokens = words.split()\n",
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "\n",
    "print(\"The tagged tokens are represented as a list of tuples:\")\n",
    "print(tagged_tokens)\n",
    "\n",
    "print(\"Now let's print them in a table!\")\n",
    "print(\"------------\")\n",
    "print(\"Token \\t Tag\")\n",
    "print(\"------------\")\n",
    "for token, tag in tagged_tokens:\n",
    "    print(token,'\\t',tag)\n",
    "print(\"------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to re-implement `enumerate()`, it would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def enumerate_clone(iterable):\n",
    "    \"This is a clone of enumerate(), which yields items and their index, one by one.\"\n",
    "    index = 0\n",
    "    for item in iterable:\n",
    "        yield (index, item)\n",
    "        index +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see whether it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../Data/RedCircle/RedCircle.txt') as f:\n",
    "    # Loop over the file, line by line.\n",
    "    for line_number, line in enumerate_clone(f):\n",
    "        if line_number == 10:\n",
    "            break\n",
    "        # Print the line\n",
    "        print(line_number, line)\n",
    "        # And break out of the loop after 10 lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to find data\n",
    "\n",
    "Here's a non-exhaustive list of places where you could get interesting data.\n",
    "\n",
    "**Curated**\n",
    "\n",
    "* Corpora (Brown ([NLTK version](http://www.nltk.org/book/ch02.html)), [OANC](http://www.anc.org/data/oanc/download/), [UMBC WebBase](http://ebiquity.umbc.edu/resource/html/id/351))\n",
    "* Psycholinguistic data (sometimes known as 'norms' in the Psychology literature)\n",
    "* DBpedia\n",
    "* Open data (e.g. [Dutch](https://data.overheid.nl/), [American](https://www.data.gov/))\n",
    "* Web N-gram data (e.g. [here](http://hpsg.fu-berlin.de/cow/ngrams/))\n",
    "\n",
    "**The web**\n",
    "\n",
    "* [USENET](http://www.psych.ualberta.ca/~westburylab/downloads/usenetcorpus.download.html)\n",
    "* [Internet Archive](https://archive.org/)\n",
    "* [Project Gutenberg](https://www.gutenberg.org/)\n",
    "* Wikipedia ([dumps](https://dumps.wikimedia.org/), [export]())\n",
    "* [Web data commons](http://webdatacommons.org/)\n",
    "\n",
    "**Do it yourself**\n",
    "\n",
    "* [BootCat](http://bootcat.sslmit.unibo.it/)\n",
    "* Experiments\n",
    "* Annotating\n",
    "* Crowdsourcing\n",
    "* ...\n",
    "\n",
    "## How to get the data\n",
    "\n",
    "### Downloading directly\n",
    "\n",
    "Here are three ways to download data from the web, each with their own use cases.\n",
    "\n",
    "* Browser (loads of data available online)\n",
    "* Command line: `wget` ([manual](https://www.gnu.org/software/wget/manual/wget.html))\n",
    "* Python: `requests`, `urllib`\n",
    "\n",
    "If you see some dataset online, or you just want to download a webpage, there is no better way than to use your browser and either save the page (from the File menu), or to right-click and press \"save as..\". But for more complex cases, you'll want to automate the process. \n",
    "\n",
    "The command line `wget` tool is like a swiss pocket knife for downloading stuff in bulk. For example, if you have a list of URLs in a text file called `list_of_urls.txt`, you can just use `wget -i list_of_urls.txt` to download all the files. You can also use the `wget` module in Python. For more complicated procedures, it's easier to just use the `requests` or `urllib` library. The `wget` tool is also [available on Windows](http://gnuwin32.sourceforge.net/packages/wget.htm). We won't explore `wget` in this course.\n",
    "\n",
    "Here is how we downloaded the Linguist List data for this course (we'll use this data later on):\n",
    "\n",
    "```python\n",
    "import os\n",
    "import urllib.request\n",
    "import time\n",
    "\n",
    "base_url = 'http://listserv.linguistlist.org/pipermail/linglite/'\n",
    "years = [str(year) for year in range(1997,2016)]\n",
    "months = ['January', 'February', 'March', 'April', 'May', 'June', 'July',\n",
    "          'August', 'September', 'October', 'November', 'December']\n",
    "\n",
    "for year in years:\n",
    "    # OS-independent way of creating the path to the folder.\n",
    "    path = os.path.join('..', 'linguistlist', year)\n",
    "    # Make the necessary folder.\n",
    "    os.makedirs(path)\n",
    "    \n",
    "    for month in months:\n",
    "        # Update variables.\n",
    "        filename = '{}-{}.txt.gz'.format(year, month)\n",
    "        path_with_file = os.path.join(path, filename)\n",
    "        url = base_url + filename\n",
    "        \n",
    "        # Write the data to disk.\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            # Use the 'wb' flag because the response contents are bytes.\n",
    "            with open(path_with_file, 'wb') as outfile:\n",
    "                data = response.read()\n",
    "                outfile.write(data)\n",
    "        \n",
    "        # Be nice to the server.\n",
    "        time.sleep(2)\n",
    "```\n",
    "\n",
    "How did we do this?\n",
    "\n",
    "* First, we went to the [Linguist List archive website](http://listserv.linguistlist.org/pipermail/linglite/). The archive looks nice, but it's a lot of work to download all of those files by hand!\n",
    "* Then, we inspected the **source** of the webpage. In Firefox, you can do this by going to `Tools/Developer/Page Source`. In Chrome: `View/Developer/View Source`. Most other browsers offer this functionality as well.\n",
    "* We saw that the URLs for the monthly archives are very regular. This is good, it means that we can exploit this regularity.\n",
    "* Then, we decided on a local structure: we want to have one folder for every year, in which all the archives for that year are stored. This structure determined the structure of our program.\n",
    "* If you don't download files often, search online for a good way to do this. Many programmers would be lost without Google/StackOverflow! The first thing we found was the `urllib` library. But a solution using the `requests` library would also be OK! That would look like this:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "# Get the data:\n",
    "r = requests.get('http://listserv.linguistlist.org/pipermail/linglite/2016-September.txt.gz')\n",
    "\n",
    "# Use the 'wb' flag because the response contents are bytes.\n",
    "with open('September.txt.gz','wb') as f:\n",
    "\t# Write the data:\n",
    "\tf.write(r.content)\n",
    "```\n",
    "\n",
    "* It's good practice to make your computer wait a little between requests. So we used the `sleep` function from the `time` module to wait 2 seconds after each download."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be surprised by the file ending: `.txt.gz`. What kind of filename is that? Basically it's an archive file, similar to a `.zip` file. You'll often see this extension for large files, because it's a means of compressing data into a smaller format. You can get the un-compressed `.txt` file by unpacking the `.txt.gz` file, but there's also a useful Python module called `gzip` that lets you inspect these files ([documentation here](https://docs.python.org/3.6/library/gzip.html)). \n",
    "\n",
    "An unzipped example is in `../Data/linguistlist/example`. Here's how to use the `gzip` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "# Open the linguist list data for the month April in the year 2000.\n",
    "# Use text-mode so that each line is returned as a unicode string.\n",
    "with gzip.open('../Data/linguistlist/2000/2000-April.txt.gz','rt') as f:\n",
    "    # Loop over the lines in the file, using enumerate to get the line numbers.\n",
    "    # Use start=1 to start counting at 1 rather than 0.\n",
    "    for line_number, line in enumerate(f, start=1):\n",
    "        # Print the line. Use end='' because lines already end in a newline character.\n",
    "        print(line, end='')\n",
    "        # Stop after 50 lines.\n",
    "        if line_number == 50:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Things to come\n",
    "This was a simple example that doesn't require us to do any parsing of the webpage itself. But how would you write a function that takes a URL like [this one](http://listserv.linguistlist.org/pipermail/linguist/2016-September/date.html) and returns all job descriptions? What would be your approach (on a high level)? \n",
    "\n",
    "We will revisit this problem below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using an API\n",
    "\n",
    "An API (*application programming interface*) provides a way for programs to interact with applications running independently. Those applications could either be running on your own computer, or they could be running somewhere else. We will be working with online APIs, specifically APIs providing the interface to some database. \n",
    "\n",
    "General guidelines for using APIs:\n",
    "\n",
    "1. Try to minimize the number of requests you make. Can you be selective before putting in your requests? \n",
    "2. Try to spread your requests so that you don't overload the server.\n",
    "3. Try to cache your results so that you don't request the same thing twice. (Think about multiple sessions and testing your code.)\n",
    "\n",
    "In short: developers providing APIs are doing us a favor. Acting nice to them is the least we can do.\n",
    "\n",
    "#### Bare APIs and wrappers\n",
    "\n",
    "APIs work like this: you send them a request (possibly with some additional information), and they send you the relevant data back. Sometimes you have to send these requests explicitly in your code, but other times there will be a *wrapper* where people have written code to provide a nice interface for you to use.\n",
    "\n",
    "**Geopy** is a nice example of a wrapper around several geolocation APIs. Read the documentation [here](https://geopy.readthedocs.io/en/1.10.0/). You can install Geopy using `pip install geopy`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the Nominatim API.\n",
    "# Read more about Nominatim here: http://wiki.openstreetmap.org/wiki/Nominatim\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "# Instantiate a geolocator object, using the Nominatim API.\n",
    "geolocator = Nominatim()\n",
    "\n",
    "# Try to find out more about a place, such as the street where the VU main building is.\n",
    "location = geolocator.geocode('de Boelelaan')\n",
    "\n",
    "# Print the place.\n",
    "print(location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** What kind of information can you get from the `Location` object?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "Here is some code to get you started if you ever want to use this API. Interesting aspects are:\n",
    "\n",
    "* Caching: this code stores the latitude and longitude for each place in a dictionary called `location_cache`. Keeping track of all responses means we never have to make the same request twice.\n",
    "* Try & Except: this code makes use of a try-except block. Typically, code following `try` is the default case, and the code following `except` is for handling situations where the code in the try-block cannot be executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('location_cache.json') as f:\n",
    "        location_cache = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    location_cache = dict()\n",
    "\n",
    "def get_lon_lat(place, location_cache):\n",
    "    \"\"\"\n",
    "    Get the latitude and longitude of a place.\n",
    "    \"\"\"\n",
    "    if place in location_cache:\n",
    "        # Get the longitude and latitude from the location cache.\n",
    "        lon, lat = location_cache[place]\n",
    "    \n",
    "    # If 'place' is not in the location cache..\n",
    "    else:\n",
    "        location = geolocator.geocode(place)\n",
    "        lon,lat  = location.longitude, location.latitude\n",
    "        location_cache[place] = [lon, lat]\n",
    "    # return longitude and latitude.\n",
    "    return lon, lat\n",
    "\n",
    "# REST OF YOUR CODE. Example:\n",
    "lon,lat = get_lon_lat('Amsterdam', location_cache)\n",
    "\n",
    "# Write out the file.\n",
    "with open('location_cache.json', 'w') as f:\n",
    "    json.dump(location_cache, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is friendly to the server, because it only makes a request if you haven't already asked where Amsterdam is. Otherwise it just returns the values from the cache. But we can make it even more friendly by making the computer wait a little between each request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "for location in ['Amsterdam', 'Utrecht', 'Amersfoort', 'Uitgeest']:\n",
    "    # Make the request. Add \", the Netherlands\" to make the request as specific as possible.\n",
    "    # This is a means to reduce errors.\n",
    "    lon, lat = get_lon_lat(location + ', the Netherlands', location_cache)\n",
    "    \n",
    "    # Do something with the result, e.g. print it.\n",
    "    print(location, 'has the following longitude and latitude:', lon, ';', lat)\n",
    "\n",
    "    # Wait.\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When there is no wrapper, you just treat the API as if you are downloading something from the URL. Let's go through some examples. Both of these provide output in JSON format.\n",
    "\n",
    "**Recipepuppy** is a website where you can search for recipes you can make with a particular set of ingredients. The description of their API is [here](http://www.recipepuppy.com/about/api/). So how do we make this work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This library comes pre-installed with Anaconda. We use it to send requests to the web.\n",
    "import requests\n",
    "\n",
    "# Get the ingredients\n",
    "ingredients = input('Please enter the ingredients as a comma-separated list.\\n')\n",
    "\n",
    "# Remove spaces if there are any. (This makes the script more robust.)\n",
    "ingredients.replace(' ','')\n",
    "\n",
    "# Prepare the API request URL\n",
    "base_url = \"http://www.recipepuppy.com/api/?i=\"\n",
    "api_request = base_url + ingredients\n",
    "\n",
    "# Get the response\n",
    "response = requests.get(api_request)\n",
    "\n",
    "# And print it\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know from last week that JSON objects are just like Python dictionaries, and you can load them using the JSON module. Let's try that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "recipe_data = json.loads(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "Woops! It turns out that data from the internet is in bytes-format. The JSON library really needs it to be a string.\n",
    "For this, we need to use the `decode` method to turn the bytes into unicode. If this sounds like magic to you, don't worry: this is something all programmers have struggled with at some point. \n",
    "\n",
    "For the next class, please watch the video [Pragmatic Unicode, or: How do I stop the pain?](http://nedbatchelder.com/text/unipain.html). And, if you want to learn more about Unicode, read [The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!)](http://www.joelonsoftware.com/articles/Unicode.html).\n",
    "\n",
    "Now, let's just convert the bytes and continue working with the recipe data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Decode bytes into utf-8 (unicode).\n",
    "decoded_data = response.content.decode('utf-8')\n",
    "\n",
    "# Load the data.\n",
    "recipe_data = json.loads(decoded_data)\n",
    "\n",
    "# Print the keys.\n",
    "print(recipe_data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked! Now I'll let you in on a secret: because conversion to text is a very common operation, there's also an attribute `response.text` that we could have used to get the recipe data in text format. But this was a useful exercise to show you how to convert from bytes to text manually.\n",
    "\n",
    "Here's the shortened version of the previous code snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the data.\n",
    "recipe_data = json.loads(response.text)\n",
    "\n",
    "# Print the keys.\n",
    "print(recipe_data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might ask: but why is there a bytes-format in the first place? Well, that's just how computers store things. And if we were to just save the recipepuppy data, there wouldn't be any need to convert it. We could just do something like this (and then you can open this file in any text editor):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Open the file in write-mode (using bytes)\n",
    "with open('recipepuppy.json','wb') as f:\n",
    "    f.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pretty printing**\n",
    "\n",
    "A nice way to inspect JSON response dictionaries is to use the built-in pretty printer from the `pprint` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the pretty printer:\n",
    "from pprint import pprint\n",
    "\n",
    "# Print the recipe data:\n",
    "pprint(recipe_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So now we understand the basics of how this API works: ingredients are passed to the website as a comma-separated string, and we get a JSON response back that we can load as a dictionary. The dictionary contains a key called 'results', which maps to a list of results (dictionaries as well). \n",
    "\n",
    "But there is more to this API. Apparently you can't just get one page of results, but you can actually get multiple pages of results. [Here](http://www.recipepuppy.com/api/?i=onions,garlic&q=omelet&p=3) is their example. Some questions:\n",
    "\n",
    "* How can you get more results?\n",
    "* How do you know whether you have *all* results for a given query?\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "Play with the URL and see what happens! Try stuff like p=500000 (or some other high number).\n",
    "We can assume that the website will give a similar page when there are no more results.\n",
    "That's when the algorithm to get all the results needs to stop.\n",
    "\n",
    "### Exercise: dealing with the Recipepuppy API\n",
    "\n",
    "We will work with [this URL for omelettes containing potatoes](http://www.recipepuppy.com/api/?i=potato&q=omelette&p=1), for the simple reason that there aren't that many recipes matching this query. It's nice to have examples like these, because you can easily test your code. Trying out all the numbers shows us that there are **three types of responses**:\n",
    "\n",
    "* http://www.recipepuppy.com/api/?i=potato&q=omelette&p=1 **Returns a JSON file with results.**\n",
    "* http://www.recipepuppy.com/api/?i=potato&q=omelette&p=2 **Gives a 404: page not found error.** (There's a bug in the API!)\n",
    "* http://www.recipepuppy.com/api/?i=potato&q=omelette&p=3 Returns a JSON file with results.\n",
    "* http://www.recipepuppy.com/api/?i=potato&q=omelette&p=4 Returns a JSON file with results.\n",
    "* http://www.recipepuppy.com/api/?i=potato&q=omelette&p=5 Returns a JSON file with results.\n",
    "* http://www.recipepuppy.com/api/?i=potato&q=omelette&p=6 **Returns a JSON file with no results.**\n",
    "\n",
    "We will write some functions to properly deal with this API. Here are all the steps:\n",
    "\n",
    "1. Write a function to return either a dictionary with the results if there is a JSON file, and `None` if the website gives an error.\n",
    "2. Write a generator function to easily loop through the result pages.\n",
    "3. Write a function to collect a specific amount of results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Part 1: write a function to get the results**\n",
    "\n",
    "Using the code below, write a function that returns either a dictionary with the results if there is a JSON file, and None if the website gives an error.\n",
    "\n",
    "HINT: loading the 404 page as a JSON string will raise an error!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_results(query, ingredients, page):\n",
    "    \"\"\"\n",
    "    Query: string indicating the kind of recipe that you're looking for.\n",
    "    Ingredients: comma-separated string of ingredients.\n",
    "    Page: results page.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE.\n",
    "    # - use a variable base_url to refer to the part of the URL that never changes.\n",
    "    # - combine the base_url with the query (e.g. 'omelette'), the ingredients \n",
    "    # (e.g. 'potato' or 'potato,onion'), and the page number (e.g. '1').\n",
    "    # - It's probably nicest if this function allows both integers as well as strings for the page number.\n",
    "    #   You can do this by converting page to a string (use str()). \n",
    "    \n",
    "    try:\n",
    "        # YOUR CODE HERE.\n",
    "        # Get the page, and load the JSON data.\n",
    "        # results = ...\n",
    "        #\n",
    "        # The results from recipepuppy.com don't have any page number.\n",
    "        # Let's fix that, because it might be useful in the future.\n",
    "        results['page'] = page\n",
    "        return results\n",
    "    except #SOME KIND OF ERROR:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Part 2: write a generator function to loop over the results**\n",
    "\n",
    "We've covered generators at the beginning of this notebook. Let's use one of them in practice! So what would a generator function for search results look like? Basically it should keep calling the API until there are no more relevant results. This is when the JSON file has an empty list of results. (In this case, we need to raise the StopIteration error because we're defining the stopping criteria ourselves. We'll give you this part of the code for free.)\n",
    "\n",
    "Please complete the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def results_generator(query, ingredients):\n",
    "    \"\"\"\n",
    "    Generator to yield all the result pages for the given query and ingredients.\n",
    "    \"\"\"\n",
    "    # Write a loop in which you keep calling the results page until there are no more results.\n",
    "    # Use the 'yield' keyword to produce the results.\n",
    "    # Be sure to also use the sleep() function to pause between calls.\n",
    "    \n",
    "    # Itertools.count() keeps counting forever if we don't do anything. \n",
    "    # The StopIteration error is there to stop once there are no more results.\n",
    "    for page_number in itertools.count(start=1):\n",
    "        \n",
    "        # YOUR CODE HERE: \n",
    "        # - get the results (call the variable 'result').\n",
    "        # - make the loop sleep.\n",
    "        \n",
    "        \n",
    "        # Some code to prevent an infinite loop while you're working. \n",
    "        # Remove this once you're sure your code works.\n",
    "        if page_number > 5:  # REMOVE THIS LINE WHEN YOU'RE DONE\n",
    "            break            # REMOVE THIS LINE WHEN YOU'RE DONE\n",
    "        \n",
    "        # If the page gives a 404 error.\n",
    "        if result == None:\n",
    "            # Continue means: move to the next iteration of the loop, \n",
    "            # without executing the rest of the code.\n",
    "            continue\n",
    "        \n",
    "        # If we got here, then that means we didn't get a None-result.\n",
    "        none_count = 0\n",
    "        \n",
    "        # If there are no more results, raise the StopIteration error so that Python knows to stop.\n",
    "        elif len(result[\"results\"]) == 0:\n",
    "            raise StopIteration\n",
    "        \n",
    "        else:\n",
    "            # YOUR CODE HERE: yield the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For testing purposes, use this code.\n",
    "results_list = []\n",
    "for result in results_generator(query=\"omelette\",ingredients=\"potato\"):\n",
    "    results_list.append(result)\n",
    "\n",
    "print(results_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2b: make the generator more robust**\n",
    "\n",
    "A problem with the generator function above is that it might produce an infinite loop if Recipepuppy.com is down. It might be a good idea to add a counter that keeps track of how many times `result` has been equal to `None`, and breaks out of the loop when that number goes over a certain threshold (say, 5 times `None` in a row). How would you do this? Modify the code to implement your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: write a function to collect a specific amount of results**\n",
    "\n",
    "Suppose you wanted to look for pasta recipes. There are hundreds of them! Getting all recipes from the API would take a long time, and you may only want to have a couple. Hence it's a good idea to write another function to get (at most) a specified number of results. Please complete the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_n_recipes(query,ingredients,n):\n",
    "    \"\"\"\n",
    "    Function that returns at most N results, where N is equal to the number of recipes.\n",
    "    \"\"\"\n",
    "    # NOTE: there are multiple recipes per results page!\n",
    "    return list_of_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More to explore\n",
    "\n",
    "**Hackernews** is a website where people can post URLs to interesting stories, submit polls, show the community something, or ask the community a question. The description of their API is [here](https://github.com/HackerNews/API). \n",
    "\n",
    "**Question**: what kind of things could you do with this data?\n",
    "\n",
    "We will use the Hackernews API in the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many APIs require you to authenticate yourself to the server, before they actually return any results. This is a means to prevent abuse (e.g. overloading the server). This usually means you have to register for the service in order to get an *API key*. We won't cover these in class (we don't want to force you to register for anything), but know there are many public APIs out there!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to process your data\n",
    "\n",
    "### Processing the data: HTML\n",
    "\n",
    "Let's take a look at a simple webpage. [Here](http://listserv.linguistlist.org/pipermail/linguist/2016-September/date.html) is one with all postings from the Linguist List in September 2016. Our goal will be to get a list with all the Job postings, including the URL. How do we go about this?\n",
    "\n",
    "Step 1. **Look at the source code first**. We can't do anything without knowing how the page is structured. You can open the page with your browser and inspect the source, or right-click the link and choose \"Save as..\" to save the file and inspect it with a text editor. What would be a good approach?\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "\n",
    "**Possible approaches**\n",
    "\n",
    "1. Use string-methods, look for all the lines with the word 'Jobs' in it, and extract the URL and title from them.\n",
    "2. Use regular expressions, write a pattern to match all links with 'Jobs' in the text.\n",
    "3. Use a module to parse the HTML first, then look for all links with the word 'Jobs'.\n",
    "\n",
    "Let me first emphasize: *There is no wrong way to do this.* If it works, it works. But as the problems you are trying to solve are getting more and more complex, it's increasingly easier to use a high-level approach. (To illustrate: how would you get the full text of [this article](http://www.bbc.com/news/disability-35881779) from the webpage? Parsing HTML is definitely the way to go, here.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2. **Create a working solution for the problem at hand.** Let's try all three approaches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Python 3 only imports libraries that it hasn't already imported.\n",
    "import requests\n",
    "\n",
    "# Get the data, and convert to string.\n",
    "response = requests.get('http://listserv.linguistlist.org/pipermail/linguist/2016-September/date.html')\n",
    "contents = response.content.decode('utf-8') # We'll use this variable as the starting point for this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, try to find all URLs and titles of job-announcements using string-methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Steps:\n",
    "# 1. Split the contents into lines.\n",
    "# 2. Create a list with all the lines containing the word 'Jobs'.\n",
    "# 3. Write a function to extract the URL and title from a line.\n",
    "# 4. Apply that function to each of the lines, and collect the results in another list.\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to find all URLs and titles of job-announcements using regular expressions. Learn about regular expressions [here](https://regexone.com/), and read the documentation for the [re](https://docs.python.org/3/library/re.html) module. Here is a small example of how to use the module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Example of how to find all smiley faces in a text.\n",
    "\n",
    "# re.compile is nice because it allows you to define a pattern wherever you want \n",
    "# (put it somewhere prominent & easy to modify) and because your code will be much\n",
    "# faster if you use the pattern often. (Otherwise Python has to compile the pattern \n",
    "# each time you want to use it.)\n",
    "\n",
    "pattern = re.compile(r':-?[\\(\\)]') # The 'r' stands for 'raw string'.\n",
    "results_1 = pattern.findall(\"\"\"Greetings! :) This is a sentence with smileys! :-) \n",
    "                The last one had a nose, probably written by an old person :(\"\"\")\n",
    "\n",
    "# Example of how to use capturing groups.\n",
    "pattern = re.compile('like (\\w+)')\n",
    "results_2 = pattern.findall(\"I like hamsters, but I don't like cleaning the cage.\")\n",
    "\n",
    "print(results_1)\n",
    "print(results_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Steps:\n",
    "# 1. Write a pattern with two capturing groups: one for the URL and one for the text.\n",
    "# 2. Use re.findall(content) to find all the job listings. You will automatically get tuples with the relevant data.\n",
    "#\n",
    "# HINT: you can use the question mark to do non-greedy matching for the asterisk. \n",
    "# '.*?\\n' will match 'everything until the end of the line'. \n",
    "# Contrast this with '.*\\n', which means \"everything up until the last line break\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's use the `lxml` module to find all URLs and titles of job announcements. See the code below for instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "\n",
    "root = html.fromstring(contents)\n",
    "\n",
    "# Modify the XPATH string so that 'links' will contain the right elements.\n",
    "# Use root.getchildren() to explore what the document tree looks like.\n",
    "# You can use getchildren() on other elements as well.\n",
    "links = root.xpath(\"./path/to/link/tag[contains(.,'Jobs')]\")\n",
    "\n",
    "# 'links' will be a list with html elements.\n",
    "# Use dir() to see what you can do with them.\n",
    "# For any link element, you can get the URL like this:\n",
    "# url = link.attrib['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3. **How generalizable is your solution?** How many steps does it take to change our solutions to, for example:\n",
    "\n",
    "* Use a different URL (maybe you want to do this in October as well).\n",
    "* Search for a different set of announcements, e.g. *Books*, or *Conferences*.\n",
    "\n",
    "You don't need to implement these changes, though you can if you want to! (Use the code boxes below.) But just read through your solutions to this problem and think about what changes should be made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing data: NLP tools\n",
    "\n",
    "The common idea for all NLP tools is that they try to structure or transform text in some meaningful way. The question of which tool you should use is only secondary to the question what you want to achieve. To give you a sense of the things you can achieve with standard NLP techniques, we will now look at two tools that you can use to analyze text: **SpaCy** and **pyspotlight**. \n",
    "\n",
    "#### SpaCy: quickly parsing documents\n",
    "\n",
    "SpaCy provides a small NLP pipeline: it takes a raw document, tokenizes it, tags all the tokens, and parses each sentence. On top of that, it also recognizes different types of entities: numbers, locations, and persons. The advantage of SpaCy is that it is really fast, and it has a good accuracy. The downside is that, at the moment, it only works for English and German. There are other tools available for different languages, but those are a bit more difficult to set up. (We can help you with this; ask us after class.)\n",
    "\n",
    "**Installing** \n",
    "\n",
    "To install SpaCy, enter the following commands on the command line.\n",
    "\n",
    "* `conda config --add channels spacy` on the command line\n",
    "* `conda install spacy`. \n",
    "* `python -m spacy.en.download` (if this doesn't work, see [here](http://spacy.io/docs/#getting-started) for updated instructions).\n",
    "\n",
    "**Using SpaCy**\n",
    "\n",
    "First let's load SpaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the English parser.\n",
    "# Note for speakers of German: it's also possible to parse German sentences using SpaCy! \n",
    "# See the documentation for more info.\n",
    "from spacy.en import English\n",
    "\n",
    "# The English parser is a class. \n",
    "# If you call it without any arguments, you will get a parser object.\n",
    "# You can use this object to parse documents.\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here's how to parse a document.\n",
    "parsed_document = parser(\"I have an awesome cat. It's sitting on the mat that I bought yesterday.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now you can loop over the document and print each sentence.\n",
    "for sentence in parsed_document.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print some information about the tokens in the second sentence.\n",
    "sentences = list(parsed_document.sents)\n",
    "for token in sentences[1]:\n",
    "    data = '\\t'.join([token.orth_,\n",
    "                      token.lemma_,\n",
    "                      token.pos_,\n",
    "                      token.tag_,\n",
    "                      str(token.i),   # Turn index into string\n",
    "                      str(token.idx)])# Turn index into string\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Question**: what is the difference between `token.pos_` and `token.tag_`? ([read the docs](https://spacy.io/docs/)) to find out.\n",
    "\n",
    "**Question:** what do the different tags mean? Read [this page](http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) to find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here's a slightly longer text, from the Wikipedia page about Harry Potter.\n",
    "harry_potter = \"\"\"Harry Potter is a series of fantasy novels written by British author J. K. Rowling. \n",
    "The novels chronicle the life of a young wizard, Harry Potter, and his friends Hermione Granger and Ron Weasley, all of whom are students at Hogwarts School of Witchcraft and Wizardry .\n",
    "The main story arc concerns Harry's struggle against Lord Voldemort, a dark wizard who intends to become immortal, overthrow the wizard governing body known as the Ministry of Magic, and subjugate all wizards and Muggles.\"\"\"\n",
    "\n",
    "sentences = parser(harry_potter)\n",
    "for e in sentences.ents:\n",
    "    first_word = list(e)[0]\n",
    "    etype = first_word.ent_type_\n",
    "    print(e,'\\t',etype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty cool, but what does NORP mean? According to the [docs](https://spacy.io/docs/#annotation-ner): Nationalities or religious or political groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pyspotlight: 'interpret' sentences using DBpedia\n",
    "\n",
    "Pyspotlight provides an easy way to use DBpedia Spotlight, which is a service you can use to find DBpedia entities in a text. DBpedia is --roughly-- a machine-readable version of Wikipedia. In short, this tool enables us to figure out which entities a text is about.\n",
    "\n",
    "**Installing**\n",
    "\n",
    "To install pyspotlight, enter the following command on the command line.\n",
    "\n",
    "* `pip install pyspotlight`\n",
    "\n",
    "**Using pyspotlight**\n",
    "\n",
    "Pyspotlight has a demo server that we can use for teaching purposes. If you'd like to use Spotlight in the future, it may be wise to set up your own server (you can run it on your laptop) or ask us to set something up for you.\n",
    "\n",
    "* Please run the code below. Is there anything surprising about the output? \n",
    "* If you speak German, Dutch, Hungarian, French, Portuguese, Italian, Russian, Turkish, or Spanish, you could try running Spotlight for any of those languages as well. See the [documentation](https://pypi.python.org/pypi/pyspotlight/0.7.1) for the list of ports in the demo server. Change `2222` below to the relevant port, and you can run Spotlight for your language!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spotlight\n",
    "\n",
    "demo_server = 'http://spotlight.sztaki.hu:2222/rest/annotate'\n",
    "\n",
    "# Annotate the Harry Potter text we've seen earlier.\n",
    "spotlight.annotate(demo_server, harry_potter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other tools (not covered in class)\n",
    "\n",
    "Unfortunately we cannot cover all NLP tools in this course. Below is a short list of tools that might be useful to you in the future. You can either use these tools as standalone programs (and then process their output using Python), or you can choose to use a *wrapper* that allows you to call these tools from inside Python.\n",
    "\n",
    "* Treetagger is a tool for tokenization and part-of-speech tagging in many languages. [Here](http://treetaggerwrapper.readthedocs.io/en/latest/) is a Python interface for it. \n",
    "* Stanford CoreNLP is a suite of NLP tools (constituting a full pipeline). [Here](https://github.com/dasmith/stanford-corenlp-python) is a library to interact with those tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Here are some exercises to help you practice your data processing skills! (These are not mandatory, but we do recommend you to try these.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK versus SpaCy\n",
    "\n",
    "There a difference in quality between SpaCy and the NLTK, with the former being superior. But how can you tell? Here's an example of both tools in action. \n",
    "\n",
    "* The example text is a case in point. What goes wrong here?\n",
    "* Try experimenting with the text to see what the differences are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Only load this cell if you haven't loaded SpaCy and nltk yet. E.g. if you restarted this notebook.\n",
    "import nltk\n",
    "from spacy.en import English\n",
    "\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = \"I like cheese very much\"\n",
    "\n",
    "print(\"NLTK results:\")\n",
    "nltk_tagged = nltk.pos_tag(text.split())\n",
    "print(nltk_tagged)\n",
    "\n",
    "print(\"SpaCy results\")\n",
    "doc = nlp(text)\n",
    "spacy_tagged = []\n",
    "for token in doc:\n",
    "    tag_data = (token.orth_, token.pos_, token.tag_,)\n",
    "    spacy_tagged.append(tag_data)\n",
    "print(spacy_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Harry Potter\n",
    "\n",
    "Use the requests library to get [the Harry Potter article from Wikipedia in JSON format]([this URL](https://en.wikipedia.org/w/api.php?action=query&prop=revisions&rvprop=content&format=json&&titles=Harry%20Potter). Then, answer the following questions:\n",
    "\n",
    "1. Who are the frequent characters mentioned in the Wikipedia article? (HINT: you might want to use SpaCy)\n",
    "2. What are the most frequent locations in the Wikipedia article?\n",
    "3. What are the most cited books on this page? What about websites? (HINT: this is a job for regular expressions!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More APIs and datasets\n",
    "\n",
    "What other APIs and datasets are available online? Use Google or some other search engine to find more APIs you could use to get interesting data. Also try to see whether the government supports open data, and what kind of data they're making available. (More and more governments do this!)\n",
    "\n",
    "Personally, I really like that the Dutch government makes all debates available in XML format. See [here](https://zoek.officielebekendmakingen.nl/zoeken/parlementaire_documenten). If you'd like to explore this data for the final assignment, I wrote some scripts [here](https://github.com/evanmiltenburg/Dutch-corpora) to download all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
