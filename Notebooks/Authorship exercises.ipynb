{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises in Authorship Detection\n",
    "\n",
    "This notebook provides a small introduction into authorship detection. It is by no means comprehensive, but there should be enough to get you started on this subject. **THIS IS A DRAFT**\n",
    "\n",
    "## Responsibility\n",
    "\n",
    "First, a point on responsibility. I want to make this very clear (especially since some of you might end up in Forensics): **Never blindly trust in a machine's judgment!** Computers are stupid, but they can easily impress humans with their ability to crunch numbers. Therefore, it's very important to manually inspect the data and the results as well. Try to understand *why* you got a particular result, and only then you can have any confidence in the conclusion.\n",
    "\n",
    "The methods illustrated in this notebook are by no means state-of-the-art, but they are foundational to the field of authorship detection.\n",
    "\n",
    "## Features\n",
    "\n",
    "By *features*, we mean properties or characteristics of a text that are useful inputs for a machine to make predictions about that text (in this case: predicting authorship). We recommend that you read [A survey of modern authorship attribution methods](http://onlinelibrary.wiley.com/doi/10.1002/asi.21001/full) by Efstathios Stamatatos. This paper provides a nice overview of features that are commonly used in authorship attribution. The paper dates back to 2008, and newer methods have been developed, but these features are still important. We'll work with a selection of features from the literature. Most important for us here is how to extract features *efficiently* and how to *represent* them. These are the features we will use:\n",
    "\n",
    "* Character counts\n",
    "* Mean word length\n",
    "* Mean sentence length\n",
    "* Standard deviation of sentence length\n",
    "* Type-token ratio\n",
    "* Hapax legomena ratio\n",
    "* Stopword counts\n",
    "* Whitespace\n",
    "\n",
    "Some of these features sound really basic, but don't let their basic nature fool you. [This piece on punctuation in novels](https://medium.com/@neuroecology/punctuation-in-novels-8f316d542ec4) shows you how informative punctuation can be. And here is an excellent quote by Gary Provost (from *100 Ways To Improve Your Writing*) that shows the power of sentence length to change the character of a text:\n",
    "\n",
    "> **VARY SENTENCE LENGTH**\n",
    "> \n",
    "> This sentence has five words. Here are five more words. Five-word sentences are fine. But several together become monotonous. Listen to what is happening. The writing is getting boring. The sound of it drones. It's like a stuck record. The ear demands some variety.\n",
    ">\n",
    "> Now listen. I vary the sentence length, and I create music. Music. The writing sings. It has a pleasant rhythm, a lilt, a harmony. I use short sentences. And I use sentences of medium length. And sometimes when I am certain the reader is rested, I will engage him with a sentence of considerable length, a sentence that burns with energy and builds with all the impetus of a crescendo, the roll of the drums, the crash of the cymbals--sounds that say listen to this, it is important.\n",
    ">\n",
    "> So write with a combination of short, medium, and long sentences. Create a sound that pleases the reader's ear. Don't just write words. Write music.\n",
    "\n",
    "(Fun fact: the use of sentence length to determine authorship goes back to [1939](http://www.jstor.org/stable/2332655?seq=1#page_scan_tab_contents)!)\n",
    "\n",
    "Of course, there are many more possible features. E.g. part-of-speech features (does the author use a lot of adjectives?) or n-gram features (what combinations of words does the author use?). Feel free to experiment with these after you've finished the exercises. Another approach is language modeling, e.g. using a Hidden Markov Model (HMM) or a Long Short-Term Memory network (LSTM). If you take this approach, the goal is to learn what sequences of tokens are typical for an author to use. With enough text, these models are really powerful.\n",
    "\n",
    "## Exercises\n",
    "\n",
    "Now let's get to work! The plan for this notebook is to write functions to extract the different features. However, we will not write a separate function for each feature! Instead, we'll group features that are naturally related to each other. Read the list of features again and try to think of the requirements each of them has: **what do you need to do in order to compute these features?**\n",
    "\n",
    "* Character counts\n",
    "* Mean word length\n",
    "* Mean sentence length\n",
    "* Median sentence length\n",
    "* Standard deviation of sentence length\n",
    "* Type-token ratio\n",
    "* Hapax legomena ratio\n",
    "* Stopword counts\n",
    "* Whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data: Reuter 50_50\n",
    "\n",
    "We're using the [Reuter 50_50 dataset](https://archive.ics.uci.edu/ml/datasets/Reuter_50_50). Here's the description of this dataset:\n",
    "\n",
    "> The dataset is the subset of RCV1. These corpus has already been used in author identification experiments. In the top 50 authors (with respect to total size of articles) were selected. 50 authors of texts labeled with at least one subtopic of the class CCAT(corporate/industrial) were selected.That way, it is attempted to minimize the topic factor in distinguishing among the texts. The training corpus consists of 2,500 texts (50 per author) and the test corpus includes other 2,500 texts (50 per author) non-overlapping with the training texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from collections import defaultdict\n",
    "\n",
    "def data_dict(path):\n",
    "    \"Get the data as a dictionary with key: author, value: list of texts.\"\n",
    "    data_by_author = defaultdict(list)\n",
    "    folders = glob.glob(path)\n",
    "    for folder in folders:\n",
    "        author = folder.split('/')[-2]\n",
    "        filenames = glob.glob(folder + '*.txt')\n",
    "        for filename in filenames:\n",
    "            with open(filename) as f:\n",
    "                data_by_author[author].append(f.read())\n",
    "    return data_by_author\n",
    "\n",
    "train_data = data_dict('../Data/authorship/C50/C50train/*/')\n",
    "test_data = data_dict('../Data/authorship/C50/C50test/*/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: LynneO'Donnell \n",
      "\n",
      "China has emerged from the cocoon of last week's National Day celebrations to swoop on the world's soybean and soymeal markets, snapping up a total of between 600,000 and 800,000 tonnes, traders said on Tuesday.\n",
      "One trader in the region took the credit for a full half of 800,000 tonnes of beans and meal business that he said had been done with Chinese buyers in the past four trading days.\n",
      "\"There has been a chunk of business going on,\" he said.\n",
      "\"I guess total meal and beans done since last Wednesday is 800,000 tonnes, with 200,000 to 300,000 of it beans and 500,000 to 600,000 tonnes of it meal,\" he said.\n",
      "Chinese buyers jumped into the market as soon as the National Day holiday was over, he said, expressing his surprise at how fast the usually cautious Chinese reacted to price falls.\n",
      "China celebrated the 47th anniversary of the founding of the communist-ruled People's Republic on October 1 with an extended public holiday that saw government, business and trade closed for three weekdays last Monday, Tuesday and Wednesday.\n",
      "During that time, soy product prices fell significantly on the Chicago Board of Trade (CBOT) on trade concerns that the U.S. crop would be higher than previously thought.\n",
      "Talk of Chinese interest has helped CBOT values move up a little in the past week.\n",
      "Reports that China last week bought between 120,000 and 150,000 tonnes of soymeal for November/December delivery from the United States, South America and India helped propel CBOT soy product futures higher on Monday. Soymeal was $2.10 per tonne higher to $0.10 lower, with October up $2.10 at $246.20 a tonne.\n",
      "The price falls coincided with one of China's peak buying periods for soybeans, soybean meal and vegetable oil.\n",
      "The regional trading source claiming half the recent business as his own said he sold meal at $300 c&amp;f (cost and freight) and beans at $5 to $10 a tonne above that.\n",
      "The soybeans were of U.S. origin, he said, meal spread between the United States and South America. Shipments would begin in October/November for December/January arrival.\n",
      "Buyers were numerous, he said, and all the purchases were for consumption.\n",
      "While the other half of the business could not be immediately confirmed, one other Asian trader said he believed a total of 600,000 tonnes of beans and meal business had been done with China since the National Day break.\n",
      "Chinese buyers were looking for low protein meal at $290 a tonne, and beans at between $290 and $300 a tonne, he said.\n",
      "\"I don't think this price will fly, but as usual PRC buyers start off low and see how low they can get it,\" he said.\n",
      "Meal business concluded since last Wednesday was 400,000 to 500,000 tonnes, he said, with beans 100,000 to 200,000 tonnes.\n",
      "Competitive freight rates in a tight market were helping keep c&amp;f values down, trading sources said, adding vessels could be found at $18 to $20 a tonne from the United States to China.\n",
      "China had been expected to buy up to 500,000 tonnes of soybeans before December, and up to 400,000 tonnes after March 1997, a Chinese trader said.\n",
      "\"That's to make sure they have enough in stock before new-crop meal comes into the market, which will be January at the earliest. And then enough in stock before Chinese New Year (in February 1997),\" he said.\n",
      "\"So we're right on target,\" he said.\n",
      "-- Hong Kong News Room (852) 2843 6470\n",
      "\n"
     ]
    }
   ],
   "source": [
    "authors = list(train_data.keys())\n",
    "first_author = authors[0]\n",
    "first_text = train_data[first_author][0]\n",
    "print(\"Example:\", first_author,'\\n')\n",
    "print(first_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools: Counter\n",
    "\n",
    "We will make use of the `Counter`-object from the `collections` module in the standard library. `Counter` is a subclass of `dict`, and has all the functionality you would expect in a dictionary, plus some cool additions. Let's first import it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to use it for counting.\n",
    "\n",
    "**Method 1**: Initialize the counter with an iterable object (string, list, set, tuple, generator). Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars = Counter('This is a string whose characters will be counted.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method 2**: After having initialized the counter, use the update method to increment the counts. Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnt = Counter()\n",
    "cnt.update(['cat','dog','cat','cat'])\n",
    "print(cnt['cat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: How can you use `chars` to get the 10 most frequent characters? (Use the docs or the `dir` and `help` functions to find out.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What do you expect Python to do if you use `cnt['mouse']`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: The built-in Python method `sum()` returns a sum of the contents of whatever iterable it is given (given those contents correspond to ints or floats). So `sum([1,2,3])` returns `6`. How can you get the sum of all counts from a counter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools: Tokenization and stopwords using the NLTK\n",
    "\n",
    "The Natural Language Toolkit (NLTK) is probably the most famous NLP library out there. Their FAQ tells the origin story:\n",
    "\n",
    "> The NLTK project began when Steven Bird was teaching CIS-530 at the University of Pennsylvania in 2001, and hired his star student, Edward Loper, from the previous offering of the course to be the teaching assistant (TA). They agreed a plan for developing software infrastructure for NLP teaching that could be easily maintained over time.\n",
    "\n",
    "Since these early beginnings, many people have contributed to the library, which now covers all areas of computational linguistics. Because of its size, we can only give you a small preview of what you can do with the NLTK. If you're hungry for more, feel free to read [the (freely available!) book](http://www.nltk.org/book/).\n",
    "\n",
    "If you are using Anaconda, NLTK is already installed. Here's how to perform sentence and word tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Use the following command to download the NLTK data, if you haven't already:\n",
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Text from the Moby Dick page on Wikipedia.\n",
    "moby_dick = \"\"\"Ishmael travels in December from Manhattan Island to New Bedford with plans to sign up for a whaling voyage. The inn where he arrives is so crowded, he must share a bed with the tattooed Polynesian Queequeg, a harpooneer whose father was king of the (fictional) island of Rokovoko. The next morning, Ishmael and Queequeg attend Father Mapple's sermon on Jonah, then head for Nantucket. Ishmael signs up with the Quaker ship-owners Bildad and Peleg for a voyage on their whaler Pequod. Peleg describes Captain Ahab: \"He's a grand, ungodly, god-like man\" who nevertheless \"has his humanities\". They hire Queequeg the following morning. A man named Elijah prophesies a dire fate should Ishmael and Queequeg join Ahab. While provisions are loaded, shadowy figures board the ship. On a cold Christmas Day, the Pequod leaves the harbor.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get all the sentences.\n",
    "sentences = sent_tokenize(moby_dick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ishmael', 'travels', 'in', 'December', 'from', 'Manhattan', 'Island', 'to', 'New', 'Bedford', 'with', 'plans', 'to', 'sign', 'up', 'for', 'a', 'whaling', 'voyage', '.']\n"
     ]
    }
   ],
   "source": [
    "# Get the words from the first sentence.\n",
    "words_from_first_sentence = word_tokenize(sentences[0])\n",
    "print(words_from_first_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools: Regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We won't need this for the current exercise. But it's generally useful to know this module to quickly extract interesting patterns.\n",
    "\n",
    "Let's first start by loading the re-module, which enables us to search using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have no experience with regular expressions before, we recommend that you follow the tutorial at [regexone.com](https://regexone.com/). If you do know regular expressions, please read the documentation on [this page](https://docs.python.org/3/library/re.html). (Note especially the part where they explain *greedy* versus *non-greedy* matching. This difference is very important to have in the back of your mind. Someday this difference *will* be relevant to you.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'cats']\n"
     ]
    }
   ],
   "source": [
    "# You can compile a pattern like this:\n",
    "pattern = re.compile('cat[a-z]*')\n",
    "\n",
    "# And find all occurrences of the pattern like this.\n",
    "# findall() returns a list of all matches of the pattern in the string.\n",
    "results = pattern.findall('I am a cat person. I have two cats, and I pet them every day.')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: how can you find all sequences of whitespace characters in a document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction\n",
    "Now we'll write the functions to extract and combine all features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Character counts**\n",
    "\n",
    "Write a function that takes a text, and produces a vector with relative character counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from string import punctuation, ascii_letters\n",
    "\n",
    "reference_chars = punctuation + ascii_letters\n",
    "\n",
    "def char_counts(text):\n",
    "    \"Function returning relative character counts for alphanumeric characters and punctuation marks.\"\n",
    "    \n",
    "    text = text.lower()\n",
    "    # First count the characters\n",
    "    \n",
    "    char_counts = # YOUR CODE HERE\n",
    "    \n",
    "    # Then determine the total amount of characters\n",
    "    total_chars = # YOUR CODE HERE\n",
    "    \n",
    "    relative_values = []\n",
    "    for char in reference_chars:\n",
    "        # YOUR CODE HERE\n",
    "    \n",
    "    # And return the relative values.\n",
    "    return relative_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Processing the text** Now we need to do some preliminary processing, so that it's possible to compute more statistics. Write a function that turns a text into a list of tokenized sentences. Example:\n",
    "\n",
    "* Input text: \"There once was a man. His name was Piek.\"\n",
    "* Output: `[['There', 'once', 'was', 'a', 'man', '.'], ['His', 'name', 'was', 'Piek', '.']]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenized_sents(text):\n",
    "    \"Function turning a text into a list of tokenized sentences.\"\n",
    "    # YOUR CODE HERE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rest of this section, let's work with this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example = \"There once was a man. His name was Piek. He was pretty good with computers!\"\n",
    "tokenized_text = tokenized_sents(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mean sentence length** Write a function to compute the mean sentence length in tokens. (You are allowed to use scipy functions.) The output should look like this: `[6.0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Median sentence length** Update the function to compute the median sentence length as well. (You are allowed to use scipy functions.) The output should now be a list with two values: `[mean, median]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standard deviation of sentence length** Update the function to also compute the standard deviation of the sentence length. (You are allowed to use scipy functions.) The output should now be a list with *three* values: [mean, median, std]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mean word length** Write a function to compute the mean word length. (You are allowed to use scipy functions.) For this function, you should exclude punctuation marks from the tokens.\n",
    "\n",
    "(Hint: you might want to use `set(punctuation)`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Type-token ratio** is the number of *different* tokens in the text divided by the *total* number of tokens. Update the word length function to also include this number. For this task, you should also exclude the punctuation marks from the tokens.\n",
    "\n",
    "(Hint: you might find the `Counter` object useful.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hapax legomena ratio** is the percentage of words only occurring once in the text, out of all the words in the vocabulary. Update the word length function to compute this ratio. \n",
    "\n",
    "Note: this is only a useful measure if you're comparing texts of similar size. See [this paper](http://www.aclweb.org/anthology/J10-4003) for explanation.\n",
    "\n",
    "(HINT: use a `Counter` object to accomplish this. You can loop over the items and see which words occur only once. Divide the number of hapaxes by the length of the word counter.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stopword counts** Count the number of times each of the stopwords in the NLTK list is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "english_stopwords = stopwords.words('english')\n",
    "\n",
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding it all together**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting authorship\n",
    "\n",
    "Sit back and relax. Your job is done! Now let's see how well we can predict who wrote which text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def train_classifier(train, train_target):\n",
    "    \"Train a random forest classifier on the given data.\"\n",
    "    rf = RandomForestClassifier(n_estimators=100)   \n",
    "    rf = rf.fit(train, train_target)\n",
    "    return rf\n",
    "\n",
    "def score_classifier(classifier, test, test_target):\n",
    "    \"This is how well the model scores on the test set.\"\n",
    "    score = classifier.score(test, test_target)\n",
    "    print(\"The model scored: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python libraries\n",
    "\n",
    "There are already several libraries to perform authorship detection. Below is a short list of libraries that I've found while researching this subject. If you want to delve deeper into authorship detection, I'd recommend you look at these in more detail. (Even if you won't use these libraries, it's nice to see how others have tackled the problem. No need to reinvent the wheel!)\n",
    "\n",
    "* Mike Kestemont's [PyStyl](https://github.com/mikekestemont/pystyl) is probably the most complete library for authorship detection.\n",
    "* The Information Sciences Institute provides the [digStylometry](https://github.com/usc-isi-i2/dig-stylometry) package.\n",
    "\n",
    "It's also possible to determine authorship using neural networks. But that goes way beyond the scope of our course.\n",
    "\n",
    "## Miscellaneous scripts\n",
    "Here are some other scripts that I've used as inspiration for the exercises in this notebook.\n",
    "\n",
    "* [Stylometry](https://github.com/jpotts18/stylometry) is a small set of scripts from Jeff Potter.\n",
    "* [Here](https://github.com/d10genes/Authorship-Attribution) is another pair of scripts from 'Chris'.\n",
    "* [Here](http://www.aicbt.com/authorship-attribution/) is a blog from a consultancy company called AICBT Consulting."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
